{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaddata():\n",
    "    post = [['my','dog','has','flea','problems','help','please'],\n",
    "           ['maybe','not','take','him','to','dog','park','stupid'],\n",
    "           ['my','dalmation','is','so','cute','I','love','him'],\n",
    "           ['stop','posting','stupid','worthless','garbage'],\n",
    "           ['mr','licks','ate','my','steak','how','to','stop','him'],\n",
    "           ['quit','buying','worthless','dog','food','stupid']]\n",
    "    classvec = [0,1,0,1,0,1]\n",
    "    return post,classvec\n",
    "def vocablist(dataset):\n",
    "    vocabset = set([])\n",
    "    for document in dataset:\n",
    "        vocabset = vocabset| set(document)\n",
    "    return list(vocabset)\n",
    "def vec(vocablist,inputset):\n",
    "    returnvec = [0]*len(vocablist)\n",
    "    for word in inputset:\n",
    "        if word in vocablist:\n",
    "            returnvec[vocablist.index(word)] = 1\n",
    "        else: print('the word:%s is not in vocablist!'%word)\n",
    "    return returnvec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "post,classvec = loaddata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocablist = vocablist(post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec(vocablist,post[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 朴素贝叶斯分类器训练函数，计算两个类别中每个单词的比例\n",
    "import numpy as np\n",
    "def train(matrix,category):\n",
    "    num = len(matrix)#文档个数\n",
    "    numword = len(matrix[0])#词汇个数\n",
    "    p = sum(category)/float(num)#class为1的概率\n",
    "    p0 = np.ones(numword)\n",
    "    p1 = np.ones(numword)\n",
    "    p0nom=2.0\n",
    "    p1nom=2.0\n",
    "    for i in range(num):\n",
    "        if category[i]==1:#文档类别为1\n",
    "            p1 += matrix[i]\n",
    "            p1nom += sum(matrix[i])\n",
    "        else:\n",
    "            p0 += matrix[i]\n",
    "            p0nom += sum(matrix[i])\n",
    "    p1vect = np.log(p1/p1nom)#class为1计算每个单词的比例\n",
    "    p0vect = np.log(p0/p0nom)#class为0计算每个单词的比例\n",
    "    return p0vect,p1vect,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainmat=[]\n",
    "for pos in post:\n",
    "    trainmat.append(vec(vocablist,pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0,p1,p = train(trainmat,classvec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(vec2,p0,p1,p):#vec2是输入的单词向量\n",
    "    p1 = np.sum(vec2*p1) + np.log(p)\n",
    "    p0 = np.sum(vec2*p0) + np.log(1-p)\n",
    "    if p1 > p0: return 1\n",
    "    else: return 0\n",
    "def test(vocablist,p0,p1,p):\n",
    "    testentry = ['love','my','dalmation']\n",
    "    thisdoc = np.array(vec(vocablist,testentry))\n",
    "    print('testentry1 classfied as:',classify(thisdoc,p0,p1,p))\n",
    "    testentry = ['stupid','garbage']\n",
    "    thisdoc = np.array(vec(vocablist,testentry))\n",
    "    print('testentry2 classified as:',classify(thisdoc,p0,p1,p))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testentry1 classfied as: 0\n",
      "testentry2 classified as: 1\n"
     ]
    }
   ],
   "source": [
    "test(vocablist,p0,p1,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#修改词集模型为词袋模型\n",
    "def bagword(vocablist,inputset):\n",
    "    returnvec = [0]*len(vocablist)\n",
    "    for word in inputset:\n",
    "        if word in vocablist:\n",
    "            returnvec[vocablist.index(word)] += 1\n",
    "    return returnvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文件解析及完整的垃圾邮件测试函数\n",
    "import re \n",
    "def  textparse(bigstring):\n",
    "    listoftoken = re.split('\\W+',bigstring)\n",
    "    return [tok.lower() for tok in listoftoken if len(tok)>2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spamtest():\n",
    "    doclist=[]\n",
    "    classlist=[]\n",
    "    fulltext=[]\n",
    "    for i in range(1,26):\n",
    "        with open('/Users/enjlife/machine-learning/machinelearninginaction/ch04/email/spam/%d.txt'% i,\n",
    "                 encoding='ISO-8859-1') as fr:\n",
    "            wordlist = textparse(fr.read())\n",
    "        doclist.append(wordlist)\n",
    "        fulltext.extend(wordlist)\n",
    "        classlist.append(1)\n",
    "        with open('/Users/enjlife/machine-learning/machinelearninginaction/ch04/email/ham/%d.txt'% i,\n",
    "                 encoding='ISO-8859-1') as fr:\n",
    "            wordlist = textparse(fr.read())\n",
    "        doclist.append(wordlist)\n",
    "        fulltext.extend(wordlist)\n",
    "        classlist.append(0)\n",
    "    vocab = vocablist(doclist)\n",
    "    trainset = list(range(50))\n",
    "    testset=[]\n",
    "    for i in range(10):\n",
    "        idx = int(np.random.uniform(0,len(trainset)))\n",
    "        testset.append(trainset[idx])\n",
    "        del(trainset[idx])\n",
    "    trainmat=[]\n",
    "    trainclass=[]\n",
    "    for idx in trainset:\n",
    "        trainmat.append(vec(vocab,doclist[idx]))\n",
    "        trainclass.append(classlist[idx])\n",
    "    p0,p1,p = train(np.array(trainmat),np.array(trainclass))\n",
    "    count = 0\n",
    "    for idx in testset:\n",
    "        wordve = np.array(vec(vocab,doclist[idx]))\n",
    "#         print(p0.shape)\n",
    "#         print(wordve.shape)\n",
    "        if classify(wordve,p0,p1,p) !=classlist[idx]:\n",
    "            print(doclist[idx])\n",
    "            count +=1\n",
    "    print('the error rate:%f'%(float(count)/len(testset)))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['benoit', 'mandelbrot', '1924', '2010', 'benoit', 'mandelbrot', '1924', '2010', 'wilmott', 'team', 'benoit', 'mandelbrot', 'the', 'mathematician', 'the', 'father', 'fractal', 'mathematics', 'and', 'advocate', 'more', 'sophisticated', 'modelling', 'quantitative', 'finance', 'died', '14th', 'october', '2010', 'aged', 'wilmott', 'magazine', 'has', 'often', 'featured', 'mandelbrot', 'his', 'ideas', 'and', 'the', 'work', 'others', 'inspired', 'his', 'fundamental', 'insights', 'you', 'must', 'logged', 'view', 'these', 'articles', 'from', 'past', 'issues', 'wilmott', 'magazine']\n",
      "the error rate:0.100000\n"
     ]
    }
   ],
   "source": [
    "spamtest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'feedparser'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-52681e28dc26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfeedparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'feedparser'"
     ]
    }
   ],
   "source": [
    "import feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
